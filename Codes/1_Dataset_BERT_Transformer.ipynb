{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1. Dataset BERT Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7193c08461ce416ca9ecf5e33af28e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5aa0412f91f5463280829369aa2baf9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8750d921faa34bff9e2eef74e3e65ac8",
              "IPY_MODEL_9600c6cc80dc411483e7d09248f412ba",
              "IPY_MODEL_5cf9d6766d254572991c6ecd0e927a89"
            ]
          }
        },
        "5aa0412f91f5463280829369aa2baf9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8750d921faa34bff9e2eef74e3e65ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d43e539dc9549368da9e057ff781b28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_126a6bdf123d453399b95239be6879b4"
          }
        },
        "9600c6cc80dc411483e7d09248f412ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e3c3d57fd384bd9b785439d5ef5d23d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67b5130f99e746d9a79651d543cd77e2"
          }
        },
        "5cf9d6766d254572991c6ecd0e927a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8786f8612fda4c2d82ef42f7ccdad304",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 437kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc8964b20fe84c55885f9d4c1d871eda"
          }
        },
        "4d43e539dc9549368da9e057ff781b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "126a6bdf123d453399b95239be6879b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e3c3d57fd384bd9b785439d5ef5d23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67b5130f99e746d9a79651d543cd77e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8786f8612fda4c2d82ef42f7ccdad304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc8964b20fe84c55885f9d4c1d871eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77ea1b86b4d94e8595215e59553178ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_44d3bc9f66aa4571a07f17dd9caf1d15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3c465b4e35ff4962a3993fafbec34495",
              "IPY_MODEL_6f3a4ad237b84eb5a3676e10bc8dd71f",
              "IPY_MODEL_9c54e49f46d7449dbc3da107b60cddf0"
            ]
          }
        },
        "44d3bc9f66aa4571a07f17dd9caf1d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c465b4e35ff4962a3993fafbec34495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_afdb5d310aeb4f868f413cf7a51e176b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cbabd320a2274650a7f07690c7b1b43e"
          }
        },
        "6f3a4ad237b84eb5a3676e10bc8dd71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76254fbc1d0142949276811e27c92fd1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 226150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 226150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a69874af2e8c4e1dbcaaa8d873738746"
          }
        },
        "9c54e49f46d7449dbc3da107b60cddf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0660cb8fd924cd1bfe743e275a82677",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 221k/221k [00:00&lt;00:00, 603kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c425c14b85a49009552bd7c508f9bfc"
          }
        },
        "afdb5d310aeb4f868f413cf7a51e176b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cbabd320a2274650a7f07690c7b1b43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76254fbc1d0142949276811e27c92fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a69874af2e8c4e1dbcaaa8d873738746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0660cb8fd924cd1bfe743e275a82677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c425c14b85a49009552bd7c508f9bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvV7Uo69gXKx"
      },
      "source": [
        "Initialization Google Drive Configuration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbwf8ZQ4msnh",
        "outputId": "18eb68a2-c5c1-4324-d286-f0cb0c521319"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaNmxQExPePh"
      },
      "source": [
        "# 1. Dataset BERT Transformer\n",
        "\n",
        "**Created By:**  Jirarote Jirasirikul\n",
        "\n",
        "**Monash University (Melbourne) Australia** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2Z_N3inRjO"
      },
      "source": [
        "This file contain a code to transform input of NLP tasks (HoC and PubMedQA) from BLURB Leaderboard into BERT vector representation.\n",
        "https://microsoft.github.io/BLURB/leaderboard.html \n",
        "\n",
        "This code has been modified from www.HuggingFace.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R7p3ovrgzrW"
      },
      "source": [
        "## Import Library\n",
        "\n",
        "All Library and File Path will be added here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLqrn9hCYWuT"
      },
      "source": [
        "# On M3 : for shell script file\n",
        "# import fire"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF5f8SlWm1pO"
      },
      "source": [
        "# Standard Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
        "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import json \n",
        "# pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVHZHjgso3hK",
        "outputId": "a2fd5293-b5bf-4d88-a9e1-2f0abdd0e81c"
      },
      "source": [
        "# BERT Transformer Library\n",
        "!pip install transformers\n",
        "import transformers as ppb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 639 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 56.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IefVNs1t6126"
      },
      "source": [
        "## Check Available Device (CPU/GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-y7tvn3W56D",
        "outputId": "6f6588e9-6c81-419e-8d29-d714ed1de6be"
      },
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    DEVICE_AVAILABLE = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    DEVICE_AVAILABLE = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkNVDHwcoIv6"
      },
      "source": [
        "## Utilities Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgAJRyveSxKi"
      },
      "source": [
        "# Customize LOGGGING function\n",
        "ENABLE_LOGS = 1\n",
        "def print_log(*arg, log_type=\"Info\"):\n",
        "    global ENABLE_LOGS\n",
        "    if(ENABLE_LOGS==1 or log_type!=\"Info\"): \n",
        "        print(\"[\"+log_type+\"]\",\" \".join(str(x) for x in arg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvG2zw2Qhzn2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##BERT Text Representation\n",
        "\n",
        "Transform Language Model\n",
        "\n",
        "When using BERT, technically we are transforming our sentence into a vector that represent each sentence. The process is call Language Model a representation of each word. \n",
        "\n",
        "BERT add [CLS] token infront of each sentence. This token representation vector could later be use for Classification as it contain the sentence representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCg6mhF8hzn2"
      },
      "source": [
        "## Define Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTf_TTDwouFl"
      },
      "source": [
        "### Class : My BERT\n",
        "This class build for assisting and store BERT data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRKtW4pvksLZ"
      },
      "source": [
        "# BERT weight Options \n",
        "# - 'distilbert-base-uncased'\n",
        "# - 'bert-base-uncased'\n",
        "# - 'dmis-lab/biobert-base-cased-v1.1'\n",
        "# - 'dmis-lab/biobert-v1.1' : Data Mining and Information Systems Lab, Korea University's picture Updated May 19 • 41k\n",
        "# - 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250,
          "referenced_widgets": [
            "7193c08461ce416ca9ecf5e33af28e16",
            "5aa0412f91f5463280829369aa2baf9f",
            "8750d921faa34bff9e2eef74e3e65ac8",
            "9600c6cc80dc411483e7d09248f412ba",
            "5cf9d6766d254572991c6ecd0e927a89",
            "4d43e539dc9549368da9e057ff781b28",
            "126a6bdf123d453399b95239be6879b4",
            "5e3c3d57fd384bd9b785439d5ef5d23d",
            "67b5130f99e746d9a79651d543cd77e2",
            "8786f8612fda4c2d82ef42f7ccdad304",
            "fc8964b20fe84c55885f9d4c1d871eda",
            "5110f03336a64baa96a7e081ef69e407",
            "17e48c95f4ec4cb98891e5e7a06ddeaf",
            "66ff4143a49945d295bc3b1e013b3865",
            "9236655dd0dd40e6be7689e1ef874e35"
          ]
        },
        "id": "cMMjcgqPSo2_",
        "outputId": "fc1bcadf-2237-4ce6-c710-1a6cca5a8d92"
      },
      "source": [
        "class my_BERT:\n",
        "    ###### Load pretrain BERT Language Model transformer (Otherwise use 'set' to customize)\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "    # Load pretrained model/tokenizer\n",
        "    bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    bert_tokenizer.add_special_tokens = True\n",
        "    bert_model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "    PRETRAIN_MAPPING = {'distilbert-base-uncased':'distilbert-base-uncased',\n",
        "                        'bert-base-uncased':'bert-base-uncased',\n",
        "                        'biobert-base-cased':'dmis-lab/biobert-base-cased-v1.1',\n",
        "                        'biobert-base-uncased':'dmis-lab/biobert-v1.1',\n",
        "                        'pubmedbert-base-uncased':'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'}\n",
        "\n",
        "    def __init__(self, df_input,is_transform=False, ENABLE_LOGS = 1):\n",
        "        ## INPUT STRUCTURE (COLUMNS): \n",
        "        ## - 'text' - Required\n",
        "        ## - 'label' - Optional default name is 'label' otherwise need to specific when called\n",
        "\n",
        "        if(is_transform):\n",
        "            self.df = None\n",
        "            self.df_BERT = df_input\n",
        "        else:\n",
        "            self.df = df_input\n",
        "            self.df_BERT = None\n",
        "        self.ENABLE_LOGS = ENABLE_LOGS\n",
        "    \n",
        "    def print_log(self, *arg, log_type=\"Info\"):\n",
        "        if(self.ENABLE_LOGS==1 or log_type!=\"Info\"): \n",
        "            print(\"[\"+log_type+\"]\",\" \".join(str(x) for x in arg))\n",
        "\n",
        "    def bert_tokenize(self, token_length=128):\n",
        "        df_output = self.df.copy()\n",
        "\n",
        "        df_output['BERTTokens'] = df_output[\"text\"].apply((lambda x: self.bert_tokenizer.encode(x, add_special_tokens=True,truncation=True)))\n",
        "        # df_output['n_tokens0'] = df_output['BERTTokens'].apply(lambda x: len(x)) # Just for verification\n",
        "        temp = df_output['BERTTokens'].apply(lambda x: len(x))\n",
        "        self.print_log(\"NO TRUNCATE\",\"Token - Done\",\"( mean/max no. of token:\",round(temp.mean()),temp.max(),\")\")\n",
        "\n",
        "        # BERT Tokenizer + truncate to BERT_MAX_LENGTH\n",
        "        df_output['BERTTokens'] = df_output[\"text\"].apply((lambda x: self.bert_tokenizer.encode(x, add_special_tokens=True,truncation=True,max_length = token_length)))\n",
        "        # df_output['n_tokens0'] = df_output['BERTTokens'].apply(lambda x: len(x)) # Just for verification\n",
        "        temp = df_output['BERTTokens'].apply(lambda x: len(x))\n",
        "        self.print_log(\"Token - Done\",\"( mean/max no. of token:\",round(temp.mean()),temp.max(),\")\")\n",
        "\n",
        "        # Padding tokens to BERT_MAX_LENGTH\n",
        "        df_output['BERTTokens'] = df_output['BERTTokens'].apply(lambda x: x + [0]*(token_length-len(x)))\n",
        "        # df_output['n_tokens'] = df_output['BERTTokens'].apply(lambda x: len(x)) # Just for verification\n",
        "        self.print_log(\"Pad - Done\")\n",
        "\n",
        "        # BERT Mask\n",
        "        df_output['BERTMasks'] = df_output['BERTTokens'].apply(lambda x: [np.where(i != 0, 1, 0) for i in x])\n",
        "        # df_output['n_mask1'] = df_output['BERTMask'].apply(lambda x: sum(x)) # Just for verification\n",
        "        self.print_log(\"Mask - Done\")\n",
        "\n",
        "        return df_output\n",
        "\n",
        "    def run_bert_transform(self, dataloader, device_available = torch.device(\"cpu\")):\n",
        "        all_result = []\n",
        "\n",
        "        self.bert_model.to(device_available)\n",
        "\n",
        "        digit = len(str(len(dataloader)))-1 # Report progress\n",
        "\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            if(step == 0 or (step+1)%(10**digit) == 0 or step == len(dataloader)-1): self.print_log(\"Step:\",step+1,\"/\",len(dataloader))\n",
        "\n",
        "            b_input_ids = batch[0].to(device_available)\n",
        "            b_input_mask = batch[1].to(device_available)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                last_hidden_states = self.bert_model(b_input_ids, attention_mask=b_input_mask)\n",
        "        \n",
        "            res_features = last_hidden_states[0][:,0,:].cpu().numpy()\n",
        "            all_result.append(res_features)\n",
        "        self.print_log(\"BERT transform - Done\")\n",
        "\n",
        "        return np.vstack(all_result)\n",
        "\n",
        "    def bert_transform(self, device_available = torch.device(\"cpu\"), batch_size = 32, token_length=128):\n",
        "        df_output = self.bert_tokenize(token_length)\n",
        "\n",
        "        # Convert to Tensor\n",
        "        input_tokens = torch.tensor(np.stack(df_output['BERTTokens'].values))\n",
        "        input_masks = torch.tensor(np.stack(df_output['BERTMasks'].values))\n",
        "        # print(input_tokens,input_masks)\n",
        "\n",
        "        # Create the DataLoader for our training set.\n",
        "        input_data = TensorDataset(input_tokens, input_masks)\n",
        "        input_sampler = SequentialSampler(input_data)\n",
        "        input_dataloader = DataLoader(input_data, sampler=input_sampler, batch_size=batch_size)\n",
        "\n",
        "        self.print_log(\"Running BERT Transform on\", str(device_available))\n",
        "        if(str(device_available) == 'cpu'):\n",
        "            self.print_log(\"Running BERT on CPU can take longer time...\",log_type=\"WARNING\")\n",
        "        self.print_log(\"BERT token length:\",token_length)\n",
        "        self.print_log(\"Data size:\",str(len(input_tokens)), \"( Total batch\", str(len(input_dataloader)),'* size',str(batch_size),\")\")\n",
        "        \n",
        "        output_features = self.run_bert_transform(input_dataloader,device_available)\n",
        "        df_output = pd.concat([df_output,pd.DataFrame(output_features.tolist()).add_prefix('feature_')],axis=1)\n",
        "        \n",
        "        self.print_log(\"BERT transformed\", log_type=\"Success\")\n",
        "        self.df_BERT = df_output\n",
        "\n",
        "    def get_features(self):\n",
        "        if(isinstance(self.df_BERT, pd.DataFrame)):\n",
        "            return np.array(self.df_BERT.filter(regex='feature_',axis=1).values)\n",
        "            # return np.array([np.array(xi) for xi in self.df_BERT.BERT_Features.values])\n",
        "        else:\n",
        "            print_log(\"Please run function 'bert_transform' to generate text representation first!\",log_type=\"Error\")\n",
        "\n",
        "    def get_labels(self, list_target = ['label']):\n",
        "        return np.array(self.df_BERT[list_target].values.tolist())\n",
        "\n",
        "    def get_current_bert_model(self):\n",
        "        return self.bert_model.config._name_or_path\n",
        "\n",
        "    def load_pretrain_bert(self, model_name='bert-base-uncased'):\n",
        "        ## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "        self.model_class, self.tokenizer_class, self.pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, self.PRETRAIN_MAPPING[model_name])\n",
        "\n",
        "        # Load pretrained model/tokenizer\n",
        "        self.bert_tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
        "        self.bert_model = self.model_class.from_pretrained(self.pretrained_weights)\n",
        "\n",
        "    def get_features_df(self,additional_col=[]):\n",
        "        if(isinstance(self.df_BERT, pd.DataFrame)):\n",
        "            return pd.concat([self.df_BERT.filter(regex='feature_',axis=1),self.df_BERT[additional_col]], axis=1)\n",
        "        else:\n",
        "            print_log(\"Please run function 'bert_transform' to generate text representation first!\",log_type=\"Error\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7193c08461ce416ca9ecf5e33af28e16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5110f03336a64baa96a7e081ef69e407",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17e48c95f4ec4cb98891e5e7a06ddeaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66ff4143a49945d295bc3b1e013b3865",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9236655dd0dd40e6be7689e1ef874e35",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6wTX6kVpf_L"
      },
      "source": [
        "## Data Transforming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1a8ydwjpnj_"
      },
      "source": [
        "### Hall Of Cancer (HoC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681Y5jJXs-M4"
      },
      "source": [
        "#### Assisting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89RkR4gHeago"
      },
      "source": [
        "## Sample code to load raw data\n",
        "\n",
        "# DATAPATH = \"/content/drive/MyDrive/MinorThesis/\"\n",
        "# DATASET = \"HoC\"\n",
        "# TOKEN_SIZE = 128\n",
        "# PRETRAIN_MODEL = 'biobert-base-uncased'\n",
        "\n",
        "# temppath_train = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"train.tsv\")\n",
        "#     # temppath_valid = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"dev.tsv\")\n",
        "#     # temppath_test = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"test.tsv\")\n",
        "\n",
        "# df_train = pd.read_csv(temppath_train, sep='\\t')\n",
        "#     # df_test = pd.read_csv(temppath_test, sep='\\t')\n",
        "#     # df_valid = pd.read_csv(temppath_valid, sep='\\t')\n",
        "\n",
        "#     # TO DO : Modify this if not HoC\n",
        "# df_train.columns = ['label','text','filename_line']\n",
        "#     # df_test.columns = ['label','text','filename_line']\n",
        "#     # df_valid.columns = ['label','text','filename_line']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ6xq74gqUfV"
      },
      "source": [
        "This function help us build a context with Previous n-sentences format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNuvC5j4flOD"
      },
      "source": [
        "def text_dependent(df_input, shift_level=0):\n",
        "    temp_df = df_input.copy()\n",
        "    new = temp_df['filename_line'].str.split(\"_\", n = 1, expand = True)\n",
        "    # making separate first name column from new data frame\n",
        "    temp_df[\"filename\"]= new[0]\n",
        "    # making separate last name column from new data frame\n",
        "    temp_df[\"sentence\"]= new[1]\n",
        "    if(shift_level==1):\n",
        "        df_input['text'] = temp_df.groupby('filename').text.apply(lambda x: x.shift(1).fillna('')+' '+ x).str.strip()\n",
        "    elif(shift_level==2):\n",
        "        df_input['text'] = temp_df.groupby('filename').text.apply(lambda x: x.shift(2).fillna('')+' '+ x.shift(1).fillna('')+' '+ x).str.strip()\n",
        "    elif(shift_level==3):\n",
        "        df_input['text'] = temp_df.groupby('filename').text.apply(lambda x: x.shift(3).fillna('')+' '+ x.shift(2).fillna('')+' '+ x.shift(1).fillna('')+' '+ x).str.strip()\n",
        "    return df_input\n",
        "\n",
        "##### Test function\n",
        "# text_dependent(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca3laQpyrtk4"
      },
      "source": [
        "This function help us transform with different parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RjM5ViTQleg"
      },
      "source": [
        "def transform_dataset(DATAPATH,DATASET,PRETRAIN_MODEL='bert-base-uncased',TOKEN_SIZE=128, SHIFT_LEVEL=None):\n",
        "    temppath_train = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"train.tsv\")\n",
        "    temppath_valid = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"dev.tsv\")\n",
        "    temppath_test = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"test.tsv\")\n",
        "\n",
        "    df_train = pd.read_csv(temppath_train, sep='\\t')\n",
        "    df_test = pd.read_csv(temppath_test, sep='\\t')\n",
        "    df_valid = pd.read_csv(temppath_valid, sep='\\t')\n",
        "\n",
        "    # TO DO : Modify this if not HoC\n",
        "    df_train.columns = ['label','text','filename_line']\n",
        "    df_test.columns = ['label','text','filename_line']\n",
        "    df_valid.columns = ['label','text','filename_line']\n",
        "\n",
        "    if(SHIFT_LEVEL != None):\n",
        "        df_train = text_dependent(df_train,SHIFT_LEVEL)\n",
        "        df_test = text_dependent(df_test,SHIFT_LEVEL)\n",
        "        df_valid = text_dependent(df_valid,SHIFT_LEVEL)\n",
        "\n",
        "    bert_train = my_BERT(df_train)\n",
        "    bert_test = my_BERT(df_test)\n",
        "    bert_valid = my_BERT(df_valid)\n",
        "\n",
        "    bert_train.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "    bert_test.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "    bert_valid.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "\n",
        "    print_log(\"BERTTransform: Train Data\")\n",
        "    bert_train.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    print_log(\"BERTTransform: Test Data\")\n",
        "    bert_test.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    print_log(\"BERTTransform: Valid Data\")\n",
        "    bert_valid.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "\n",
        "    if(SHIFT_LEVEL == None or SHIFT_LEVEL == 0):\n",
        "        temp_path = os.path.join(DATAPATH,\"datasets\",\"transformed\",DATASET,PRETRAIN_MODEL,\"token_length_\"+str(TOKEN_SIZE))\n",
        "    else:\n",
        "        temp_path = os.path.join(DATAPATH,\"datasets\",\"transformed\",DATASET,PRETRAIN_MODEL,\"token_length_\"+str(TOKEN_SIZE)+\"_shift_\"+str(SHIFT_LEVEL))\n",
        "    Path(os.path.join(temp_path)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    temp_df = bert_train.get_features_df(['filename_line','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"train.csv\"))\n",
        "    print_log(len(df_train),\"/\",len(temp_df))\n",
        "    temp_df = bert_test.get_features_df(['filename_line','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"test.csv\"))\n",
        "    print_log(len(df_test),\"/\",len(temp_df))\n",
        "    temp_df = bert_valid.get_features_df(['filename_line','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"valid.csv\"))\n",
        "    print_log(len(df_valid),\"/\",len(temp_df))\n",
        "\n",
        "    return bert_train, bert_test, bert_valid\n",
        "\n",
        "# res = transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "#     DATASET = \"HoC\",\n",
        "#     TOKEN_SIZE = 512,\n",
        "#     PRETRAIN_MODEL = 'biobert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8snN3MmtDFb"
      },
      "source": [
        "#### Executing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzE7GwSQXrNn"
      },
      "source": [
        "# # For Execute in M3\n",
        "# if __name__ == \"__main__\":\n",
        "#     fire.Fire(transform_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e2c5O9ZctChq",
        "outputId": "c744afe5-0ad4-4766-cf59-803b6ae5e961"
      },
      "source": [
        "for bert_type in ['bert-base-uncased','pubmedbert-base-uncased','biobert-base-cased']:\n",
        "    for shift in range(4):\n",
        "        res = transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "                                DATASET = \"HoC\",\n",
        "                                TOKEN_SIZE = 512,\n",
        "                                PRETRAIN_MODEL = bert_type,\n",
        "                                SHIFT_LEVEL = shift)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 40 230 )\n",
            "[Info] Token - Done ( mean/max no. of token: 40 230 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 12119 ( Total batch 379 * size 32 )\n",
            "[Info] Step: 1 / 379\n",
            "[Info] Step: 100 / 379\n",
            "[Info] Step: 200 / 379\n",
            "[Info] Step: 300 / 379\n",
            "[Info] Step: 379 / 379\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 40 150 )\n",
            "[Info] Token - Done ( mean/max no. of token: 40 150 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 3547 ( Total batch 111 * size 32 )\n",
            "[Info] Step: 1 / 111\n",
            "[Info] Step: 100 / 111\n",
            "[Info] Step: 111 / 111\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Valid Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 41 228 )\n",
            "[Info] Token - Done ( mean/max no. of token: 41 228 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 1798 ( Total batch 57 * size 32 )\n",
            "[Info] Step: 1 / 57\n",
            "[Info] Step: 10 / 57\n",
            "[Info] Step: 20 / 57\n",
            "[Info] Step: 30 / 57\n",
            "[Info] Step: 40 / 57\n",
            "[Info] Step: 50 / 57\n",
            "[Info] Step: 57 / 57\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 12119 / 12119\n",
            "[Info] 3547 / 3547\n",
            "[Info] 1798 / 1798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 74 284 )\n",
            "[Info] Token - Done ( mean/max no. of token: 74 284 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 12119 ( Total batch 379 * size 32 )\n",
            "[Info] Step: 1 / 379\n",
            "[Info] Step: 100 / 379\n",
            "[Info] Step: 200 / 379\n",
            "[Info] Step: 300 / 379\n",
            "[Info] Step: 379 / 379\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 74 242 )\n",
            "[Info] Token - Done ( mean/max no. of token: 74 242 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 3547 ( Total batch 111 * size 32 )\n",
            "[Info] Step: 1 / 111\n",
            "[Info] Step: 100 / 111\n",
            "[Info] Step: 111 / 111\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Valid Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 76 284 )\n",
            "[Info] Token - Done ( mean/max no. of token: 76 284 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 1798 ( Total batch 57 * size 32 )\n",
            "[Info] Step: 1 / 57\n",
            "[Info] Step: 10 / 57\n",
            "[Info] Step: 20 / 57\n",
            "[Info] Step: 30 / 57\n",
            "[Info] Step: 40 / 57\n",
            "[Info] Step: 50 / 57\n",
            "[Info] Step: 57 / 57\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 12119 / 12119\n",
            "[Info] 3547 / 3547\n",
            "[Info] 1798 / 1798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 104 367 )\n",
            "[Info] Token - Done ( mean/max no. of token: 104 367 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 12119 ( Total batch 379 * size 32 )\n",
            "[Info] Step: 1 / 379\n",
            "[Info] Step: 100 / 379\n",
            "[Info] Step: 200 / 379\n",
            "[Info] Step: 300 / 379\n",
            "[Info] Step: 379 / 379\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 104 317 )\n",
            "[Info] Token - Done ( mean/max no. of token: 104 317 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 3547 ( Total batch 111 * size 32 )\n",
            "[Info] Step: 1 / 111\n",
            "[Info] Step: 100 / 111\n",
            "[Info] Step: 111 / 111\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Valid Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 108 331 )\n",
            "[Info] Token - Done ( mean/max no. of token: 108 331 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 1798 ( Total batch 57 * size 32 )\n",
            "[Info] Step: 1 / 57\n",
            "[Info] Step: 10 / 57\n",
            "[Info] Step: 20 / 57\n",
            "[Info] Step: 30 / 57\n",
            "[Info] Step: 40 / 57\n",
            "[Info] Step: 50 / 57\n",
            "[Info] Step: 57 / 57\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 12119 / 12119\n",
            "[Info] 3547 / 3547\n",
            "[Info] 1798 / 1798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-18b98d1e8bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mTOKEN_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mPRETRAIN_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                 SHIFT_LEVEL = shift)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-02b722ca7ca0>\u001b[0m in \u001b[0;36mtransform_dataset\u001b[0;34m(DATAPATH, DATASET, PRETRAIN_MODEL, TOKEN_SIZE, SHIFT_LEVEL)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERTTransform: Train Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mbert_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE_AVAILABLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOKEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERTTransform: Test Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbert_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE_AVAILABLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOKEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-62d489afa9fc>\u001b[0m in \u001b[0;36mbert_transform\u001b[0;34m(self, device_available, batch_size, token_length)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbert_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mdf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Convert to Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-62d489afa9fc>\u001b[0m in \u001b[0;36mbert_tokenize\u001b[0;34m(self, token_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BERTTokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# df_output['n_tokens0'] = df_output['BERTTokens'].apply(lambda x: len(x)) # Just for verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BERTTokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-62d489afa9fc>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BERTTokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# df_output['n_tokens0'] = df_output['BERTTokens'].apply(lambda x: len(x)) # Just for verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BERTTokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2173\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2176\u001b[0m         )\n\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2509\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2512\u001b[0m         )\n\u001b[1;32m   2513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m             )\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2IjzvFFEOju"
      },
      "source": [
        "## Select out of the results to Preview label count for each cancer types\n",
        "# dev = res[2]\n",
        "# for i in range(10):\n",
        "#    print(dev.df['label'].str.split(',',expand = True)[i].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anz9eYreZ97s"
      },
      "source": [
        "### PubMedQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9qxUK5CMA8K"
      },
      "source": [
        "def pubmedqa_transform_dataset(DATAPATH,DATASET,PRETRAIN_MODEL='bert-base-uncased',TOKEN_SIZE=128, REASONING=False):\n",
        "    list_data_fold = []\n",
        "    for i in range(1): # We merge dataset to generate trained (Separate later using index - filename_line)\n",
        "        print(\"fold\",i)\n",
        "        temppath_train = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"pqal_fold\"+str(i),\"train_set.json\")\n",
        "        temppath_valid = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"pqal_fold\"+str(i),\"dev_set.json\")\n",
        "            \n",
        "        df_temp_train = pd.read_json(temppath_train).transpose()\n",
        "        df_temp_valid = pd.read_json(temppath_valid).transpose()\n",
        "        list_data_fold.append((df_temp_train,df_temp_valid))\n",
        "        print(df_temp_train.shape,df_temp_valid.shape)\n",
        "        # print(df_temp_train.value_counts(\"final_decision\")/450*100)\n",
        "        # print(df_temp_valid.value_counts(\"final_decision\")/50*100)\n",
        "\n",
        "    # Test\n",
        "    temppath_test = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"test_set.json\")\n",
        "    df_train = pd.concat([list_data_fold[0][0],list_data_fold[0][1]])\n",
        "    df_test = pd.read_json(temppath_test).transpose()\n",
        "\n",
        "    print(\"Final\",df_train.shape,df_test.shape)        \n",
        "    return list_data_fold,df_test\n",
        "\n",
        "\n",
        "# list_data_fold,df_test = pubmedqa_transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "#             DATASET = \"pubmedqa\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcgpzN3KgZrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "77ea1b86b4d94e8595215e59553178ca",
            "44d3bc9f66aa4571a07f17dd9caf1d15",
            "3c465b4e35ff4962a3993fafbec34495",
            "6f3a4ad237b84eb5a3676e10bc8dd71f",
            "9c54e49f46d7449dbc3da107b60cddf0",
            "afdb5d310aeb4f868f413cf7a51e176b",
            "cbabd320a2274650a7f07690c7b1b43e",
            "76254fbc1d0142949276811e27c92fd1",
            "a69874af2e8c4e1dbcaaa8d873738746",
            "d0660cb8fd924cd1bfe743e275a82677",
            "2c425c14b85a49009552bd7c508f9bfc",
            "d67ae4e1d3f4406dae2c3c03baa816a7",
            "49bb4b2c748541d3ab99d6454ab785b7",
            "494ce0cad6e543f8b8c0551f0c518b8f",
            "0acbcc16803b44f4bcd3091e624ecd25",
            "5c5ff2a9e7d94880a125a9e9a685bea3",
            "89528d14903b475e813dfb8d013174f2"
          ]
        },
        "outputId": "38cf1b06-d2b3-4c4e-a851-bf423502a8b1"
      },
      "source": [
        "def pubmedqa_transform_dataset(DATAPATH,DATASET,PRETRAIN_MODEL='bert-base-uncased',TOKEN_SIZE=128, REASONING=False,POSITION=\"QuesAbs\"):\n",
        "    # Train & Valid\n",
        "    list_data_fold = []\n",
        "    for i in range(1): # We merge dataset to generate trained (Separate later using index - filename_line)\n",
        "        temppath_train = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"pqal_fold\"+str(i),\"train_set.json\")\n",
        "        temppath_valid = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"pqal_fold\"+str(i),\"dev_set.json\")\n",
        "        \n",
        "        df_temp_train = pd.read_json(temppath_train).transpose()\n",
        "        df_temp_valid = pd.read_json(temppath_valid).transpose()\n",
        "        list_data_fold.append((df_temp_train,df_temp_valid))\n",
        "        print(df_temp_train.shape,df_temp_valid.shape)\n",
        "\n",
        "    # Test\n",
        "    temppath_test = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"test_set.json\")\n",
        "    df_train = pd.concat([list_data_fold[0][0],list_data_fold[0][1]])\n",
        "    df_test = pd.read_json(temppath_test).transpose()\n",
        "\n",
        "    print(\"Final\",df_train.shape,df_test.shape)\n",
        "\n",
        "    # # TO DO : Modify this if not HoC\n",
        "\n",
        "    \n",
        "    if(REASONING):\n",
        "        # REASONING REQUIRED\n",
        "        df_train_mod = df_train[['QUESTION','CONTEXTS','final_decision','reasoning_required_pred','reasoning_free_pred']].reset_index().copy()\n",
        "        if(POSITION == \"QuesAbs\"):\n",
        "            df_train_mod['text'] = df_train_mod.QUESTION +\". \"+ df_train_mod.CONTEXTS.apply(lambda x : (' ').join(x)) #question before\n",
        "        else:\n",
        "            df_train_mod['text'] = df_train_mod.QUESTION +\". \"+ df_train_mod.CONTEXTS.apply(lambda x : (' ').join(x)) #question before\n",
        "        \n",
        "        # df_train_mod['text'] = df_train_mod.CONTEXTS.apply(lambda x : (' ').join(x)) +\" \"+ df_train_mod.QUESTION  #question after\n",
        "        df_train_mod.drop(columns=['QUESTION','CONTEXTS'],inplace=True)\n",
        "        df_train_mod.columns = ['id','label','reasoning_required_pred','reasoning_free_pred','text']\n",
        "\n",
        "        df_test_mod = df_test[['QUESTION','CONTEXTS','final_decision','reasoning_required_pred','reasoning_free_pred']].reset_index().copy()\n",
        "        if(POSITION == \"QuesAbs\"):\n",
        "            df_test_mod['text'] = df_test_mod.QUESTION +\". \"+ df_test_mod.CONTEXTS.apply(lambda x : (' ').join(x)) #question before\n",
        "        else:\n",
        "            df_test_mod['text'] = df_test_mod.CONTEXTS.apply(lambda x : (' ').join(x)) +\" \"+ df_test_mod.QUESTION  #question after\n",
        "\n",
        "        df_test_mod.drop(columns=['QUESTION','CONTEXTS'],inplace=True)\n",
        "        df_test_mod.columns = ['id','label','reasoning_required_pred','reasoning_free_pred','text']\n",
        "    else:\n",
        "        # REASONING FREE\n",
        "        df_train_mod = df_train[['QUESTION','LONG_ANSWER','final_decision','reasoning_required_pred','reasoning_free_pred']].reset_index().copy()\n",
        "        if(POSITION == \"QuesAbs\"):\n",
        "            df_train_mod['text'] = df_train_mod.QUESTION +\" \"+ df_train_mod.LONG_ANSWER #question before\n",
        "        else:\n",
        "            df_train_mod['text'] = df_train_mod.LONG_ANSWER +\" \"+ df_train_mod.QUESTION #question after\n",
        "\n",
        "        df_train_mod.drop(columns=['QUESTION','LONG_ANSWER'],inplace=True)\n",
        "        df_train_mod.columns = ['id','label','reasoning_required_pred','reasoning_free_pred','text']\n",
        "\n",
        "        df_test_mod = df_test[['QUESTION','LONG_ANSWER','final_decision','reasoning_required_pred','reasoning_free_pred']].reset_index().copy()\n",
        "        if(POSITION == \"QuesAbs\"):\n",
        "            df_test_mod['text'] = df_test_mod.QUESTION +\" \"+ df_test_mod.LONG_ANSWER #question before\n",
        "        else:\n",
        "            df_test_mod['text'] = df_test_mod.LONG_ANSWER +\" \"+ df_test_mod.QUESTION #question after\n",
        "          \n",
        "        df_test_mod.drop(columns=['QUESTION','LONG_ANSWER'],inplace=True)\n",
        "        df_test_mod.columns = ['id','label','reasoning_required_pred','reasoning_free_pred','text']\n",
        "\n",
        "\n",
        "    bert_train = my_BERT(df_train_mod)\n",
        "    bert_test = my_BERT(df_test_mod)\n",
        "\n",
        "    bert_train.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "    bert_test.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "\n",
        "    print_log(\"BERTTransform: Train Data\")\n",
        "    bert_train.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    print_log(\"BERTTransform: Test Data\")\n",
        "    bert_test.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "\n",
        "    temp_path = os.path.join(DATAPATH,\"datasets\",\"transformed\",DATASET,\"QuesAbs\",\"reasoning_required\" if REASONING else \"reasoning_free\",PRETRAIN_MODEL,\"token_length_\"+str(TOKEN_SIZE))\n",
        "    Path(os.path.join(temp_path)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    temp_df = bert_train.get_features_df(['id','label','reasoning_required_pred','reasoning_free_pred'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"train.csv\"))\n",
        "    print_log(len(df_train),\"/\",len(temp_df))\n",
        "    temp_df = bert_test.get_features_df(['id','label','reasoning_required_pred','reasoning_free_pred'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"test.csv\"))\n",
        "    print_log(len(df_test),\"/\",len(temp_df))\n",
        "\n",
        "    return bert_train, bert_test\n",
        "\n",
        "# df_train,df_test = pubmedqa_transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "#             DATASET = \"pubmedqa\",\n",
        "#             TOKEN_SIZE = 512,\n",
        "#             PRETRAIN_MODEL = 'pubmedbert-base-uncased',\n",
        "#             REASONING=True)\n",
        "\n",
        "for i in [True,False]:\n",
        "    for j in ['pubmedbert-base-uncased','bert-base-uncased','biobert-base-cased']:\n",
        "        df_train,df_test = pubmedqa_transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "            DATASET = \"pubmedqa\",\n",
        "            TOKEN_SIZE = 512,\n",
        "            PRETRAIN_MODEL = j,\n",
        "            REASONING=i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77ea1b86b4d94e8595215e59553178ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/221k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d67ae4e1d3f4406dae2c3c03baa816a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49bb4b2c748541d3ab99d6454ab785b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "494ce0cad6e543f8b8c0551f0c518b8f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 293 611 )\n",
            "[Info] Token - Done ( mean/max no. of token: 292 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 299 613 )\n",
            "[Info] Token - Done ( mean/max no. of token: 299 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n",
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 325 512 )\n",
            "[Info] Token - Done ( mean/max no. of token: 325 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 331 512 )\n",
            "[Info] Token - Done ( mean/max no. of token: 331 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n",
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0acbcc16803b44f4bcd3091e624ecd25",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c5ff2a9e7d94880a125a9e9a685bea3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/313 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89528d14903b475e813dfb8d013174f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 343 690 )\n",
            "[Info] Token - Done ( mean/max no. of token: 340 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 349 683 )\n",
            "[Info] Token - Done ( mean/max no. of token: 346 512 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n",
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 66 150 )\n",
            "[Info] Token - Done ( mean/max no. of token: 66 150 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 65 173 )\n",
            "[Info] Token - Done ( mean/max no. of token: 65 173 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n",
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 76 173 )\n",
            "[Info] Token - Done ( mean/max no. of token: 76 173 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 74 196 )\n",
            "[Info] Token - Done ( mean/max no. of token: 74 196 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n",
            "(450, 9) (50, 9)\n",
            "Final (500, 9) (500, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERTTransform: Train Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 80 174 )\n",
            "[Info] Token - Done ( mean/max no. of token: 80 174 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] BERTTransform: Test Data\n",
            "[Info] NO TRUNCATE Token - Done ( mean/max no. of token: 78 210 )\n",
            "[Info] Token - Done ( mean/max no. of token: 78 210 )\n",
            "[Info] Pad - Done\n",
            "[Info] Mask - Done\n",
            "[Info] Running BERT Transform on cuda\n",
            "[Info] BERT token length: 512\n",
            "[Info] Data size: 500 ( Total batch 16 * size 32 )\n",
            "[Info] Step: 1 / 16\n",
            "[Info] Step: 10 / 16\n",
            "[Info] Step: 16 / 16\n",
            "[Info] BERT transform - Done\n",
            "[Success] BERT transformed\n",
            "[Info] 500 / 500\n",
            "[Info] 500 / 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBCxD6RdZ87f"
      },
      "source": [
        "# # For test in Colab Only\n",
        "# datapath = \"/content/drive/MyDrive/MinorThesis/\"\n",
        "# dataset = \"pubmedqa\"\n",
        "# token_size = 128\n",
        "# pretrain_model = 'biobert-base-uncased'\n",
        "\n",
        "# # Train & Valid\n",
        "# list_data_fold = []\n",
        "# for i in range(10):\n",
        "#     temppath_train = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"pqal_fold\"+str(i),\"train_set.json\")\n",
        "#     temppath_valid = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"pqal_fold\"+str(i),\"dev_set.json\")\n",
        "    \n",
        "#     df_temp_train = pd.read_json(temppath_train).transpose()\n",
        "#     df_temp_valid = pd.read_json(temppath_valid).transpose()\n",
        "#     list_data_fold.append((df_temp_train,df_temp_valid))\n",
        "#     print(df_temp_train.shape,df_temp_valid.shape)\n",
        "\n",
        "# # Test\n",
        "# temppath_test = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"test_set.json\")\n",
        "# df_test = pd.read_json(temppath_test).transpose()\n",
        "\n",
        "# # Test Label\n",
        "# temppath_test_gt = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"test_ground_truth.json\")\n",
        "# df_test_label = pd.read_json(temppath_test_gt, typ='series')\n",
        "\n",
        "# # # Original 1k data before split\n",
        "# # temppath_ori = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"ori_pqal.json\")\n",
        "# # df_ori = pd.read_json(temppath_ori).transpose()\n",
        "# # df_ori.shape\n",
        "\n",
        "# print(\"Final\",list_data_fold[0][0].shape,list_data_fold[0][1].shape,df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmNVBhse8EGk"
      },
      "source": [
        "# # Extract Fold ID (test,train)\n",
        "\n",
        "# def get_pubmedqa_fold_id():\n",
        "#     list_data_fold = []\n",
        "#     for i in range(10):\n",
        "#         temppath_train = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"pqal_fold\"+str(i),\"train_set.json\")\n",
        "#         temppath_valid = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"pqal_fold\"+str(i),\"dev_set.json\")\n",
        "        \n",
        "#         df_temp_train = pd.read_json(temppath_train).transpose().reset_index()\n",
        "#         df_temp_valid = pd.read_json(temppath_valid).transpose().reset_index()\n",
        "\n",
        "#         list_data_fold.append((df_temp_train['index'].values,df_temp_valid['index'].values))\n",
        "#         # list_data_fold.append((df_temp_train,df_temp_valid))\n",
        "#         # print(df_temp_train.shape,df_temp_valid.shape)\n",
        "#     return list_data_fold\n",
        "    \n",
        "# list_data_fold = get_pubmedqa_fold_id()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6jM_MIWzI_g"
      },
      "source": [
        "### BioASQ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDUg-iVIzIcp"
      },
      "source": [
        "def bioasq_transform_dataset(DATAPATH,DATASET,PRETRAIN_MODEL='bert-base-uncased',TOKEN_SIZE=128, REASONING=False):\n",
        "    # Train & Valid & Test\n",
        "    temppath_train = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"train.tsv\")\n",
        "    temppath_valid = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"dev.tsv\")\n",
        "    temppath_test = os.path.join(DATAPATH,\"datasets\",\"raw\",DATASET,\"test.tsv\")\n",
        "        \n",
        "    df_train = pd.read_csv(temppath_train,sep=\"\\t\",header=None)\n",
        "    df_valid = pd.read_csv(temppath_valid,sep=\"\\t\",header=None)\n",
        "    df_test = pd.read_csv(temppath_test,sep=\"\\t\",header=None)\n",
        "\n",
        "    df_train.columns = [\"id\",\"question\",\"answer\",\"label\"]\n",
        "    df_valid.columns = [\"id\",\"question\",\"answer\",\"label\"]\n",
        "    df_test.columns = [\"id\",\"question\",\"answer\",\"label\"]\n",
        "\n",
        "    print(\"Final\",df_train.shape,df_valid.shape,df_test.shape)\n",
        "\n",
        "    # # TO DO : Modify this if not HoC\n",
        "    df_train['text'] = df_train.question +\". \"+ df_train.answer\n",
        "    df_valid['text'] = df_valid.question +\". \"+ df_valid.answer\n",
        "    df_test['text'] = df_test.question +\". \"+ df_test.answer\n",
        "\n",
        "    bert_train = my_BERT(df_train)\n",
        "    bert_test = my_BERT(df_test)\n",
        "    bert_valid = my_BERT(df_valid)\n",
        "\n",
        "    bert_train.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "    bert_test.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "    bert_valid.load_pretrain_bert(PRETRAIN_MODEL)\n",
        "\n",
        "    print_log(\"BERTTransform: Train Data\")\n",
        "    bert_train.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    print_log(\"BERTTransform: Test Data\")\n",
        "    bert_test.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    print_log(\"BERTTransform: Valid Data\")\n",
        "    bert_valid.bert_transform(DEVICE_AVAILABLE, token_length=TOKEN_SIZE)\n",
        "    \n",
        "    temp_path = os.path.join(DATAPATH,\"datasets\",\"transformed\",DATASET,PRETRAIN_MODEL,\"token_length_\"+str(TOKEN_SIZE))\n",
        "    Path(os.path.join(temp_path)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Additional labels for HOC\n",
        "\n",
        "    temp_df = bert_train.get_features_df(['id','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"train.csv\"))\n",
        "    print_log(len(df_train),\"/\",len(temp_df))\n",
        "    temp_df = bert_test.get_features_df(['id','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"test.csv\"))\n",
        "    print_log(len(df_test),\"/\",len(temp_df))\n",
        "    temp_df = bert_valid.get_features_df(['id','label'])\n",
        "    temp_df.to_csv(os.path.join(temp_path,\"valid.csv\"))\n",
        "    print_log(len(df_valid),\"/\",len(temp_df))\n",
        "\n",
        "    # return bert_train, bert_test\n",
        "\n",
        "# bioasq_transform_dataset(DATAPATH = \"/content/drive/MyDrive/MinorThesis/\",\n",
        "#             DATASET = \"BioASQ\",\n",
        "#             TOKEN_SIZE = 512,\n",
        "#             PRETRAIN_MODEL = 'pubmedbert-base-uncased',\n",
        "#             REASONING=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSRAzwFkz0hc"
      },
      "source": [
        "# # For test in Colab Only\n",
        "# datapath = \"/content/drive/MyDrive/MinorThesis/\"\n",
        "# dataset = \"BioASQ\"\n",
        "# token_size = 512\n",
        "# pretrain_model = 'biobert-base-uncased'\n",
        "\n",
        "# # Train & Valid\n",
        "# temppath_train = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"train.tsv\")\n",
        "# temppath_valid = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"dev.tsv\")\n",
        "# temppath_test = os.path.join(datapath,\"datasets\",\"raw\",dataset,\"test.tsv\")\n",
        "    \n",
        "# df_train = pd.read_csv(temppath_train,sep=\"\\t\",header=None)\n",
        "# df_valid = pd.read_csv(temppath_valid,sep=\"\\t\",header=None)\n",
        "# df_test = pd.read_csv(temppath_test,sep=\"\\t\",header=None)\n",
        "\n",
        "# df_train.columns = [\"id\",\"question\",\"answer\",\"label\"]\n",
        "# df_valid.columns = [\"id\",\"question\",\"answer\",\"label\"]\n",
        "# df_test.columns = [\"id\",\"question\",\"answer\",\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVogjV2M0uI2"
      },
      "source": [
        "# df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iViOXFtR016c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}